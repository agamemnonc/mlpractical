{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial is about linear transforms - a basic building block of neural networks, including deep learning models.\n",
    "\n",
    "# Virtual environments and syncing repositories\n",
    "\n",
    "Before you proceed onwards, remember to activate you virtual environments so you can use the software you installed last week as well as run the notebooks in interactive mode, not through the github.com website.\n",
    "\n",
    "## Virtual environments\n",
    "\n",
    "To activate the virtual environment:\n",
    "   * If you were in last week's Tuesday or Wednesday group type `activate_mlp` or `source ~/mlpractical/venv/bin/activate`\n",
    "   * If you were in the Monday group:\n",
    "      + and if you have chosen the **comfy** way type: `workon mlpractical`\n",
    "      + and if you have chosen the **generic** way, `source` your virutal environment using `source` and specyfing the path to the activate script (you need to localise it yourself, there were not any general recommendations w.r.t dir structure and people have installed it in different places, usually somewhere in the home directories. If you cannot easily find it by yourself, use something like: `find . -iname activate` ):\n",
    "\n",
    "## On Synchronising repositories\n",
    "\n",
    "Enter the git mlp repository you set up last week (i.e. `~/mlpractical/repo-mlp`) and once you sync the repository (in one of the two below ways), start the notebook session by typing:\n",
    "\n",
    "```\n",
    "ipython notebook\n",
    "```\n",
    "\n",
    "### Default way\n",
    "\n",
    "To avoid potential conflicts between the changes you have made since last week and our additions, we recommend `stash` your changes and `pull` the new code from the mlpractical repository by typing:\n",
    "\n",
    "1. `git stash save \"Lab1 work\"`\n",
    "2. `git pull`\n",
    "\n",
    "Then, if you need to, you can always (temporaily) restore a desired state of the repository.\n",
    "\n",
    "### For advanced github users\n",
    "\n",
    "It is OK if you want to keep your changes and merge the new code with whatever you already have, but you need to know what you are doing and how to resolve conflicts.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Layer Models\n",
    "\n",
    "***\n",
    "### Note on storing matrices in computer memory\n",
    "\n",
    "Suppose you want to store the following matrix in memory: $\\left[ \\begin{array}{ccc}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "7 & 8 & 9 \\end{array} \\right]$ \n",
    "\n",
    "If you allocate the memory at once for the whole matrix, then the above matrix would be organised as a vector in one of two possible forms:\n",
    "\n",
    "* Row-wise layout where the order would look like: $\\left [ \\begin{array}{ccccccccc}\n",
    "1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\end{array} \\right ]$\n",
    "* Column-wise layout where the order would look like: $\\left [ \\begin{array}{ccccccccc}\n",
    "1 & 4 & 7 & 2 & 5 & 8 & 3 & 6 & 9 \\end{array} \\right ]$\n",
    "\n",
    "Although `numpy` can easily handle both formats (possibly with some computational overhead), in our code we will stick with the more modern (and default) `C`-like approach and use the row-wise format (in contrast to Fortran that used a column-wise approach). \n",
    "\n",
    "This means, that in this tutorial:\n",
    "* vectors are kept row-wise $\\mathbf{x} = (x_1, x_1, \\ldots, x_D) $ (rather than $\\mathbf{x} = (x_1, x_1, \\ldots, x_D)^T$)\n",
    "* similarly, in case of matrices we will stick to: $\\left[ \\begin{array}{cccc}\n",
    "x_{11} & x_{12} & \\ldots & x_{1D} \\\\\n",
    "x_{21} & x_{22} & \\ldots & x_{2D} \\\\\n",
    "x_{31} & x_{32} & \\ldots & x_{3D} \\\\ \\end{array} \\right]$ and each row (i.e. $\\left[ \\begin{array}{cccc} x_{11} & x_{12} & \\ldots & x_{1D} \\end{array} \\right]$) represents a single data-point (like one MNIST image or one window of observations)\n",
    "\n",
    "In lecture slides you will find the equations following the conventional mathematical approach, using column vectors, but you can easily map between column-major and row-major organisations using a matrix transpose.\n",
    "\n",
    "***\n",
    "\n",
    "## Linear and Affine Transforms\n",
    "\n",
    "The basis of all linear models is the so called affine transform, which is a transform that implements a linear transformation and translation of the input features. The transforms we are going to use are parameterised by:\n",
    "\n",
    "  * A weight matrix $\\mathbf{W} \\in \\mathbb{R}^{D\\times K}$: where element $w_{ik}$ is the weight from input $x_i$ to output $y_k$\n",
    "  * A bias vector $\\mathbf{b}\\in R^{K}$ : where element $b_{k}$ is the bias for output $k$\n",
    "\n",
    "Note, the bias is simply some additive term, and can be easily incorporated into an additional row in weight matrix and an additional input in the inputs which is set to $1.0$ (as in the below picture taken from the lecture slides). However, here (and in the code) we will keep them separate.\n",
    "\n",
    "![Making Predictions](res/singleLayerNetWts-1.png)\n",
    "\n",
    "For instance, for the above example of 5-dimensional input vector by $\\mathbf{x} = (x_1, x_2, x_3, x_4, x_5)$, weight matrix $\\mathbf{W}=\\left[ \\begin{array}{ccc}\n",
    "w_{11} & w_{12} & w_{13} \\\\\n",
    "w_{21} & w_{22} & w_{23} \\\\\n",
    "w_{31} & w_{32} & w_{33} \\\\\n",
    "w_{41} & w_{42} & w_{43} \\\\\n",
    "w_{51} & w_{52} & w_{53} \\\\ \\end{array} \\right]$, bias vector $\\mathbf{b} = (b_1, b_2, b_3)$ and outputs $\\mathbf{y} = (y_1, y_2, y_3)$, one can write the transformation as follows:\n",
    "\n",
    "(for the $i$-th output)\n",
    "\n",
    "(1) $\n",
    "\\begin{equation}\n",
    "   y_i = b_i + \\sum_j x_jw_{ji}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "or the equivalent vector form (where $\\mathbf w_i$ is the $i$-th row of $\\mathbf W$):\n",
    "\n",
    "(2) $\n",
    "\\begin{equation}\n",
    "   y_i = b_i + \\mathbf x \\mathbf w_i^T\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "The same operation can be also written in matrix form, to compute all the outputs $\\mathbf{y}$ at the same time:\n",
    "\n",
    "(3) $\n",
    "\\begin{equation}\n",
    "  \\mathbf y=\\mathbf x\\mathbf W + \\mathbf b\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "This is equivalent to slides 12/13 in lecture 1, except we are using row vectors.\n",
    "\n",
    "When $\\mathbf{x}$ is a mini-batch (contains $B$ data-points of dimension $D$ each), i.e. $\\left[ \\begin{array}{cccc}\n",
    "x_{11} & x_{12} & \\ldots & x_{1D} \\\\\n",
    "x_{21} & x_{22} & \\ldots & x_{2D} \\\\\n",
    "\\cdots \\\\\n",
    "x_{B1} & x_{B2} & \\ldots & x_{BD} \\\\ \\end{array} \\right]$ equation (3) effectively becomes to be\n",
    "\n",
    "(4) $\n",
    "\\begin{equation}\n",
    "  \\mathbf Y=\\mathbf X\\mathbf W + \\mathbf b\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "where both $\\mathbf{X}\\in\\mathbb{R}^{B\\times D}$ and $\\mathbf{Y}\\in\\mathbb{R}^{B\\times K}$ are matrices, and $\\mathbf{b}$ needs to be <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\">broadcasted</a> $B$ times (numpy will do this by default). However, we will not make an explicit distinction between a special case for $B=1$ and $B>1$ and simply use equation (3) instead, although $\\mathbf{x}$ and hence $\\mathbf{y}$ could be matrices. From an implementation point of view, it does not matter.\n",
    "\n",
    "The desired functionality for matrix multiplication in numpy is provided by <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html\">numpy.dot</a> function. If you haven't use it so far, get familiar with it as we will use it extensively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product of a and b is 10\n",
      "The element-wise product of a and b is [3 4 3]\n"
     ]
    }
   ],
   "source": [
    "# Dot products\n",
    "import numpy as np\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([3,2,1])\n",
    "print \"The dot product of a and b is\", np.dot(a,b)\n",
    "print \"The element-wise product of a and b is\", a*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A general note on random number generators\n",
    "\n",
    "It is generally a good practice (for machine learning applications **not** for cryptography!) to seed a pseudo-random number generator once at the beginning of the experiment, and use it later through the code where necesarry. This makes it easier to reproduce results since random initialisations can be replicated. As such, within this course we are going use a single random generator object, similar to the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialise the random generator to be used later\n",
    "seed=[2015, 10, 1]\n",
    "random_generator = numpy.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 \n",
    "\n",
    "Using `numpy.dot`, implement **forward** propagation through the linear transform defined by equations (3) and (4) for $B=1$ and $B>1$. As data ($\\mathbf{x}$) use `MNISTDataProvider` introduced last week. For the case when $B=1$, write a function to compute the 1st output ($y_1$) using equations (1) and (2). Check if the output is the same as the corresponding one obtained with numpy. \n",
    "\n",
    "Tip: To generate random data you can use `random_generator.uniform(-0.1, 0.1, (D, 10))` from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mlp.dataset import MNISTDataProvider\n",
    "\n",
    "mnist_dp = MNISTDataProvider(dset='valid', batch_size=3, max_num_batches=1, randomize=False)\n",
    "\n",
    "irange = 0.1\n",
    "W = random_generator.uniform(-irange, irange, (784,10)) \n",
    "b = numpy.zeros((10,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1e1 -0.0442595171786\n",
      "y1e1 -0.0442595171786\n",
      "ye3 [[-0.04  0.92 -0.4   0.94  0.2  -0.12  0.67 -0.04 -0.22 -0.04]\n",
      " [ 0.3   0.43 -0.57 -0.29 -0.89 -0.16  0.64  0.39 -0.24 -0.16]\n",
      " [ 0.07  0.05 -0.11 -0.5  -0.2  -0.23  0.18  0.81 -0.48  0.05]]\n",
      "ye4 [[-0.04  0.92 -0.4   0.94  0.2  -0.12  0.67 -0.04 -0.22 -0.04]\n",
      " [ 0.3   0.43 -0.57 -0.29 -0.89 -0.16  0.64  0.39 -0.24 -0.16]\n",
      " [ 0.07  0.05 -0.11 -0.5  -0.2  -0.23  0.18  0.81 -0.48  0.05]]\n"
     ]
    }
   ],
   "source": [
    "mnist_dp.reset()\n",
    "\n",
    "#implement following functions, then run the cell\n",
    "def y1_equation_1(x, W, b):\n",
    "    y1 = b[0]\n",
    "    for j in range(len(x)):\n",
    "        y1 += x[j]*W[j,0]\n",
    "    return y1\n",
    "    \n",
    "def y1_equation_2(x, W, b):\n",
    "    y1 = b[0] + np.dot(x, W[:,0])\n",
    "    return y1\n",
    "\n",
    "def y_equation_3(x, W, b):\n",
    "    #use numpy.dot\n",
    "    y = np.zeros((np.shape(x)[0], np.shape(W)[1]))\n",
    "    for j in range(np.shape(x)[0]): # Each data point separately\n",
    "        y[j,:] = np.dot(x[j,:],W) + b\n",
    "    return y\n",
    "\n",
    "def y_equation_4(x, W, b):\n",
    "    #use numpy.dot\n",
    "    y = np.dot(x,W) + b # All together\n",
    "    return y\n",
    "\n",
    "for x, t in mnist_dp:\n",
    "    y1e1 = y1_equation_1(x[0], W, b)\n",
    "    y1e2 = y1_equation_2(x[0], W, b)\n",
    "    ye3 = y_equation_3(x, W, b)\n",
    "    ye4 = y_equation_4(x, W, b)\n",
    "\n",
    "print 'y1e1', y1e1\n",
    "print 'y1e1', y1e1\n",
    "print 'ye3', ye3\n",
    "print 'ye4', ye4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n",
      "(3, 784) \n",
      "(3, 10) \n",
      "\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print mnist_dp._curr_idx\n",
    "mnist_dp.reset()\n",
    "x, t = mnist_dp.next() # This is how we get access to a single batch (the current one)\n",
    "print np.shape(x), '\\n', np.shape(t), '\\n'\n",
    "print mnist_dp._curr_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "Modify the examples from Exercise 1 to perform **backward** propagation, that is, given $\\mathbf{y}$ (obtained in the previous step) and weight matrix $\\mathbf{W}$, project $\\mathbf{y}$ onto the input space $\\mathbf{x}$ (ignore or set to zero the biases towards $\\mathbf{x}$ in backward pass). Mathematically, we are interested in the following transformation: $\\mathbf{z}=\\mathbf{y}\\mathbf{W}^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def x_equation_bck(y, W):\n",
    "    x = np.dot(y, W.T)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_star = x_equation_bck(y1e1,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.  0. -0. ..., -0.  0.  0.]\n",
      " [-0. -0.  0. ..., -0. -0. -0.]\n",
      " [ 0. -0.  0. ..., -0. -0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0. -0.]\n",
      " [-0. -0. -0. ...,  0.  0.  0.]\n",
      " [-0.  0.  0. ...,  0. -0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print x_star[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 3 (optional)\n",
    "\n",
    "In case you do not fully understand how matrix-vector and/or matrix-matrix products work, consider implementing `my_dot_mat_mat` function  (you have been given `my_dot_vec_mat` code to look at as an example) which takes as the input the following arguments:\n",
    "\n",
    "* D-dimensional input vector $\\mathbf{x} = (x_1, x_2, \\ldots, x_D) $.\n",
    "* Weight matrix $\\mathbf{W}\\in\\mathbb{R}^{D\\times K}$:\n",
    "\n",
    "and returns:\n",
    "\n",
    "* K-dimensional output vector $\\mathbf{y} = (y_1, \\ldots, y_K) $\n",
    "\n",
    "Your job is to write a variant that works in a mini-batch mode where both $\\mathbf{x}\\in\\mathbb{R}^{B\\times D}$ and $\\mathbf{y}\\in\\mathbb{R}^{B\\times K}$ are matrices in which each rows contain one of $B$ data-points from mini-batch (rather than  $\\mathbf{x}\\in\\mathbb{R}^{D}$ and $\\mathbf{y}\\in\\mathbb{R}^{K}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def my_dot_vec_mat(x, W):\n",
    "    D = x.shape[0]\n",
    "    K = W.shape[1]\n",
    "    assert (D == W.shape[0]), (\n",
    "        \"Number of columns of x expected to \"\n",
    "        \" to be equal to the number of rows in \"\n",
    "        \"W, but got shapes %s, %s\" % (x.shape, W.shape)\n",
    "    )\n",
    "    y = numpy.zeros((K,))\n",
    "    for k in xrange(0, K):\n",
    "        for d in xrange(0, D):\n",
    "            y[k] += x[d] * W[d,k]\n",
    "                \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "irange = 0.1 #+-range from which we draw the random numbers\n",
    "\n",
    "x = random_generator.uniform(-irange, irange, (5,)) \n",
    "W = random_generator.uniform(-irange, irange, (5,3)) \n",
    "\n",
    "y_my = my_dot_vec_mat(x, W)\n",
    "y_np = numpy.dot(x, W)\n",
    "\n",
    "same = numpy.allclose(y_my, y_np)\n",
    "\n",
    "if same:\n",
    "    print 'Well done!'\n",
    "else:\n",
    "    print 'Matrices are different:'\n",
    "    print 'y_my is: ', y_my\n",
    "    print 'y_np is: ', y_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_dot_mat_mat(x, W):\n",
    "    B = x.shape[0]\n",
    "    D = x.shape[1]\n",
    "    K = W.shape[1]\n",
    "    assert (D == W.shape[0]), (\n",
    "        \"Number of columns in of x expected to \"\n",
    "        \" to be the same as rows in W, but got shapes %s %s\" % (x.shape, W.shape)\n",
    "    )\n",
    "    #allocate the output container\n",
    "    y = numpy.zeros((B, K))\n",
    "    \n",
    "    #implement here matrix-matrix inner product here\n",
    "    for b in xrange(B):\n",
    "        for k in xrange(K):\n",
    "            for d in range(D):\n",
    "                y[b,k] += x[b,d]*W[d,k]\n",
    "                \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether you get comparable numbers to what numpy is producing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "irange = 0.1 #+-range from which we draw the random numbers\n",
    "\n",
    "x = random_generator.uniform(-irange, irange, (2,5)) \n",
    "W = random_generator.uniform(-irange, irange, (5,3)) \n",
    "\n",
    "y_my = my_dot_mat_mat(x, W)\n",
    "y_np = numpy.dot(x, W)\n",
    "\n",
    "same = numpy.allclose(y_my, y_np)\n",
    "\n",
    "if same:\n",
    "    print 'Well done!'\n",
    "else:\n",
    "    print 'Matrices are different:'\n",
    "    print 'y_my is: ', y_my\n",
    "    print 'y_np is: ', y_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we benchmark each approach (we do it in separate cells, as timeit currently can measure whole cell execuiton only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate bit bigger matrices, to better evaluate timings\n",
    "x = random_generator.uniform(-irange, irange, (10, 1000))\n",
    "W = random_generator.uniform(-irange, irange, (1000, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_dot timings:\n",
      "10 loops, best of 3: 744 ms per loop\n"
     ]
    }
   ],
   "source": [
    "print 'my_dot timings:'\n",
    "%timeit -n10 my_dot_mat_mat(x, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy.dot timings:\n",
      "10 loops, best of 3: 159 Âµs per loop\n"
     ]
    }
   ],
   "source": [
    "print 'numpy.dot timings:'\n",
    "%timeit -n10 numpy.dot(x, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional section ends here**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative learning of linear models\n",
    "\n",
    "We will learn the model with stochastic gradient descent on N data-points using mean square error (MSE) loss function, which is defined as follows:\n",
    "\n",
    "(5) $ \\normalsize\n",
    "E = \\frac{1}{2} \\sum_{n=1}^N ||\\mathbf{y}^n - \\mathbf{t}^n||^2 =  \\sum_{n=1}^N E^n \\\\\n",
    "  E^n = \\frac{1}{2} ||\\mathbf{y}^n - \\mathbf{t}^n||^2\n",
    "$\n",
    "\n",
    "(6) $ \\normalsize E^n = \\frac{1}{2} \\sum_{k=1}^K (y_k^n - t_k^n)^2 $\n",
    "  \n",
    "Hence, the gradient w.r.t (with respect to) the $r$ output y of the model is defined as, so called delta function, $\\delta_r$: \n",
    "\n",
    "(8) $\\normalsize \\frac{\\partial{E^n}}{\\partial{y_{r}}} = (y^n_r - t^n_r) =  \\delta^n_r \\quad ; \\quad\n",
    "    \\delta^n_r = y^n_r - t^n_r \\\\\n",
    "    \\frac{\\partial{E}}{\\partial{y_{r}}} = \\sum_{n=1}^N \\frac{\\partial{E^n}}{\\partial{y_{r}}} = \\sum_{n=1}^N \\delta^n_r\n",
    "$\n",
    "\n",
    "Also: $ \\frac{\\partial{y_r^n}}{\\partial{w_{sr}}} = x_s^n$\n",
    "\n",
    "Similarly, using the above $\\delta^n_r$ one can express the gradient of the  weight $w_{sr}$ (from the s-th input to the r-th output) for linear model and MSE cost as follows:\n",
    "\n",
    "(9) $ \\normalsize\n",
    "    \\frac{\\partial{E^n}}{\\partial{w_{sr}}} = \\frac{\\partial{E^n}}{\\partial{y_{r}^n}} \\frac{\\partial{y_r^n}}{\\partial{w_{sr}}} = (y^n_r - t^n_r)x_s^n =  \\delta^n_r x_s^n \\quad\\\\\n",
    "    \\frac{\\partial{E}}{\\partial{w_{sr}}} = \\sum_{n=1}^N \\frac{\\partial{E^n}}{\\partial{w_{rs}}} = \\sum_{n=1}^N \\delta^n_r x_s^n\n",
    "$\n",
    "\n",
    "and the gradient for bias parameter at the $r$-th output is:\n",
    "\n",
    "(10) $ \\normalsize\n",
    "    \\frac{\\partial{E}}{\\partial{b_{r}}} = \\sum_{n=1}^N \\frac{\\partial{E^n}}{\\partial{b_{r}}} = \\sum_{n=1}^N \\delta^n_r\n",
    "$ (since the corresponding $x_0$ is always equal to 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Making Predictions](res/singleLayerNetPredict.png)\n",
    " \n",
    "  * Input vector $\\mathbf{x} = (x_1, x_2, \\ldots, x_D) $\n",
    "  * Output scalar $y_1$\n",
    "  * Weight matrix $\\mathbf{W}$: $w_{ik}$ is the weight from input $x_i$ to output $y_k$. Note, here this is really a vector since a single scalar output, y_1.\n",
    "  * Scalar bias $b$ for the only output in our model \n",
    "  * Scalar target $t$ for the only output in out model\n",
    "  \n",
    "First, ensure you can make use of the data provider (note, for training data has been normalised to zero mean and unit variance, hence different effective range than one can find in file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: \n",
      "[[-0.11  0.35  0.08 -0.13  0.39  0.13 -0.05 -0.07 -0.09 -0.12]\n",
      " [ 0.35  0.08 -0.13  0.39  0.13 -0.05 -0.07 -0.09 -0.12 -0.08]\n",
      " [ 0.08 -0.13  0.39  0.13 -0.05 -0.07 -0.09 -0.12 -0.08 -0.06]\n",
      " [-0.13  0.39  0.13 -0.05 -0.07 -0.09 -0.12 -0.08 -0.06 -0.04]\n",
      " [ 0.39  0.13 -0.05 -0.07 -0.09 -0.12 -0.08 -0.06 -0.04 -0.13]\n",
      " [ 0.13 -0.05 -0.07 -0.09 -0.12 -0.08 -0.06 -0.04 -0.13 -0.13]\n",
      " [-0.05 -0.07 -0.09 -0.12 -0.08 -0.06 -0.04 -0.13 -0.13 -0.13]\n",
      " [-0.07 -0.09 -0.12 -0.08 -0.06 -0.04 -0.13 -0.13 -0.13 -0.1 ]\n",
      " [-0.09 -0.12 -0.08 -0.06 -0.04 -0.13 -0.13 -0.13 -0.1   0.08]\n",
      " [-0.12 -0.08 -0.06 -0.04 -0.13 -0.13 -0.13 -0.1   0.08 -0.06]]\n",
      "To predict: \n",
      "[[-0.08]\n",
      " [-0.06]\n",
      " [-0.04]\n",
      " [-0.13]\n",
      " [-0.13]\n",
      " [-0.13]\n",
      " [-0.1 ]\n",
      " [ 0.08]\n",
      " [-0.06]\n",
      " [ 0.04]]\n",
      "Observations: \n",
      "[[-0.08 -0.06 -0.04 -0.13 -0.13 -0.13 -0.1   0.08 -0.06  0.04]\n",
      " [-0.06 -0.04 -0.13 -0.13 -0.13 -0.1   0.08 -0.06  0.04  0.12]\n",
      " [-0.04 -0.13 -0.13 -0.13 -0.1   0.08 -0.06  0.04  0.12 -0.1 ]\n",
      " [-0.13 -0.13 -0.13 -0.1   0.08 -0.06  0.04  0.12 -0.1  -0.12]\n",
      " [-0.13 -0.13 -0.1   0.08 -0.06  0.04  0.12 -0.1  -0.12 -0.03]\n",
      " [-0.13 -0.1   0.08 -0.06  0.04  0.12 -0.1  -0.12 -0.03  0.12]\n",
      " [-0.1   0.08 -0.06  0.04  0.12 -0.1  -0.12 -0.03  0.12  0.5 ]\n",
      " [ 0.08 -0.06  0.04  0.12 -0.1  -0.12 -0.03  0.12  0.5  -0.08]\n",
      " [-0.06  0.04  0.12 -0.1  -0.12 -0.03  0.12  0.5  -0.08  0.22]\n",
      " [ 0.04  0.12 -0.1  -0.12 -0.03  0.12  0.5  -0.08  0.22 -0.03]]\n",
      "To predict: \n",
      "[[ 0.12]\n",
      " [-0.1 ]\n",
      " [-0.12]\n",
      " [-0.03]\n",
      " [ 0.12]\n",
      " [ 0.5 ]\n",
      " [-0.08]\n",
      " [ 0.22]\n",
      " [-0.03]\n",
      " [-0.11]]\n"
     ]
    }
   ],
   "source": [
    "from mlp.dataset import MetOfficeDataProvider\n",
    "\n",
    "modp = MetOfficeDataProvider(window_size = 10, batch_size=10, max_num_batches=2, randomize=False)\n",
    "\n",
    "%precision 2\n",
    "for x, t in modp:\n",
    "    print 'Observations: \\n', x\n",
    "    print 'To predict: \\n', t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "The below code implements a very simple variant of stochastic gradient descent for the rainfall prediction example. Your task is to implement 5 functions in the next cell and then run two next cells that 1) build sgd functions and 2) run the actual training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder:\n",
    "\n",
    "B: number of examples in each batch\n",
    "\n",
    "D: input dimensionality\n",
    "\n",
    "K: output dimensionality\n",
    "\n",
    "* $ X \\in \\mathbb{R}^{B\\times D}$\n",
    "* $W \\in \\mathbb{R}^{D\\times K}$,\n",
    "* $ Y \\in \\mathbb{R}^{B\\times K}$,\n",
    "* $ b \\in \\mathbb{R}^{K}$ (bias)\n",
    "\n",
    "It might be the case that $B = 1$, so we need to take this into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#When implementing those, take into account the mini-batch case, for which one is\n",
    "#expected to sum the errors for each example\n",
    "\n",
    "def fprop(x, W, b):\n",
    "    #code implementing eq. (3)\n",
    "    # I need to fix the asserts to account for the case where B = 1 (by looking into ndim(x))\n",
    "    assert (x.shape[1] == W.shape[0]), (\n",
    "        \"Number of columns in of x expected to \"\n",
    "        \" to be the same as rows in W, but got shapes %s %s\" % (x.shape, W.shape)\n",
    "    )\n",
    "    \n",
    "    assert (b.shape[0] == W.shape[1]), (\n",
    "        \"Number of columns in of x expected to \"\n",
    "        \" to be the same as rows in W, but got shapes %s %s\" % (x.shape, W.shape)\n",
    "    )\n",
    "    \n",
    "    y = np.dot(x,W) + b\n",
    "    return y\n",
    "\n",
    "def cost(y, t):\n",
    "    #Mean Square Error cost, equation (5)\n",
    "    assert (y.shape == t.shape), (\n",
    "        \"The vectos y and t are expected to have same shapes \"\n",
    "        \" but got shapes %s %s\" % (y.shape, t.shape)\n",
    "    )\n",
    "    cost_sum = np.sum(np.linalg.norm((y-t), axis = 0), axis = 0)\n",
    "    \n",
    "    return 0.5*cost_sum\n",
    "\n",
    "            \n",
    "def cost_grad(y, t):\n",
    "    #Gradient of the cost w.r.t y equation (8)\n",
    "    assert (y.shape == t.shape), (\n",
    "        \"The vectos y and t are expected to have same shapes \"\n",
    "        \" but got shapes %s %s\" % (y.shape, t.shape)\n",
    "    )\n",
    "    cost_gradient = y-t\n",
    "    #cost_gradient = np.sum(y-t, axis = 0) # Total gradient\n",
    "    return cost_gradient # B X K\n",
    "\n",
    "def cost_wrt_W(cost_grad, x):\n",
    "    #Gradient of the cost w.r.t W, equation (9)\n",
    "    B, D = x.shape\n",
    "    K = cost_grad.shape[1] # Gradient of E wrt y\n",
    "    \n",
    "    cost_gradient_w = np.zeros((D, K))\n",
    "    for ss in xrange(D):\n",
    "        for rr in xrange(K):\n",
    "            cost_gradient_w[ss,rr] =  np.dot(x[:,ss],cost_grad[:,rr])\n",
    "    \n",
    "    return cost_gradient_w\n",
    "        \n",
    "def cost_wrt_b(cost_grad):\n",
    "    #Gradient of the cost w.r.t to b, equation (10)\n",
    "    K = cost_grad.shape[1] # Gradient of E wrt y\n",
    "    cost_gradient_b = np.zeros(K)\n",
    "    tmp = cost_grad\n",
    "    for rr in xrange(K):\n",
    "        cost_gradient_b[rr] = sum(tmp[:,rr])\n",
    "    \n",
    "    return cost_gradient_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 1.86407968411\n",
      "Shape of y and t: (10, 1)\n",
      "Gradient w.r.t y: \n",
      "[[ 1.67]\n",
      " [-1.56]\n",
      " [ 1.38]\n",
      " [-0.75]\n",
      " [ 1.2 ]\n",
      " [ 0.7 ]\n",
      " [-1.03]\n",
      " [ 1.38]\n",
      " [ 0.21]\n",
      " [-1.12]]\n",
      "Gradient shape: (10, 1)\n",
      "Gradient w.r.t W: \n",
      "[[-0.1 ]\n",
      " [-0.62]\n",
      " [ 0.23]\n",
      " [ 0.17]\n",
      " [-0.51]\n",
      " [ 0.02]\n",
      " [-0.79]\n",
      " [ 0.39]\n",
      " [ 0.21]\n",
      " [-0.68]]\n",
      "Griadent shape: (10, 1)\n",
      "Gradient w.r.t b: \n",
      "[ 2.08]\n",
      "Griadent shape: (1,)\n"
     ]
    }
   ],
   "source": [
    "# Testing (1-dimensional output)\n",
    "y = t + np.random.randn(y.shape[0],y.shape[1]) # B X 1\n",
    "print 'Cost:', cost(y,t)\n",
    "print 'Shape of y and t:', y.shape\n",
    "print 'Gradient w.r.t y: \\n', cost_grad(y,t)\n",
    "print 'Gradient shape:', cost_grad(y,t).shape\n",
    "print 'Gradient w.r.t W: \\n', cost_wrt_W(cost_grad(y,t), x)\n",
    "print 'Griadent shape:', cost_wrt_W(cost_grad(y,t), x).shape\n",
    "print 'Gradient w.r.t b: \\n', cost_wrt_b(cost_grad(y,t))\n",
    "print 'Griadent shape:', cost_wrt_b(cost_grad(y,t)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 2.91478562047\n",
      "Shape of y and t: (10, 2)\n",
      "Gradient w.r.t y: \n",
      "[[ 0.6  -0.3 ]\n",
      " [ 0.35 -0.73]\n",
      " [ 1.57 -0.11]\n",
      " [-1.31  1.6 ]\n",
      " [-0.59 -0.64]\n",
      " [-1.14  1.33]\n",
      " [ 0.43  0.38]\n",
      " [-1.02  1.81]\n",
      " [-0.76 -1.56]\n",
      " [ 0.41  0.51]]\n",
      "Gradient shape: (10, 2)\n",
      "Gradient w.r.t W: \n",
      "[[ 0.19  0.01]\n",
      " [ 0.23 -0.27]\n",
      " [-0.33 -0.11]\n",
      " [-0.24  0.17]\n",
      " [-0.15  0.39]\n",
      " [ 0.1  -0.01]\n",
      " [-0.04 -0.21]\n",
      " [-0.43 -0.5 ]\n",
      " [ 0.09  1.05]\n",
      " [ 0.06 -0.41]]\n",
      "Griadent shape: (10, 2)\n",
      "Gradient w.r.t b: \n",
      "[-1.46  2.28]\n",
      "Griadent shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "# Testing (2-dimensional output, B X K)\n",
    "t_qwe=np.hstack((t,t))\n",
    "print 'Cost:', cost(y_qwe,t_qwe)\n",
    "print 'Shape of y and t:', t_qwe.shape\n",
    "y_qwe = t_qwe + np.random.randn(t_qwe.shape[0],t_qwe.shape[1])\n",
    "print 'Gradient w.r.t y: \\n', cost_grad(y_qwe,t_qwe)\n",
    "print 'Gradient shape:', cost_grad(y_qwe,t_qwe).shape\n",
    "print 'Gradient w.r.t W: \\n', cost_wrt_W(cost_grad(y_qwe,t_qwe), x)\n",
    "print 'Griadent shape:', cost_wrt_W(cost_grad(y_qwe,t_qwe), x).shape\n",
    "print 'Gradient w.r.t b: \\n', cost_wrt_b(cost_grad(y_qwe,t_qwe))\n",
    "print 'Griadent shape:', cost_wrt_b(cost_grad(y_qwe,t_qwe)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_epoch(data_provider, W, b, learning_rate):\n",
    "    mse_stats = []\n",
    "    \n",
    "    #get the minibatch of data\n",
    "    for x, t in data_provider:\n",
    "        \n",
    "        #1. get the estimate of y\n",
    "        y = fprop(x, W, b)\n",
    "\n",
    "        #2. compute the loss function\n",
    "        tmp = cost(y, t)\n",
    "        mse_stats.append(tmp)\n",
    "        \n",
    "        #3. compute the grad of the cost w.r.t the output layer activation y\n",
    "        #i.e. how the cost changes when output y changes\n",
    "        cost_grad_deltas = cost_grad(y, t)\n",
    "\n",
    "        #4. compute the gradients w.r.t model's parameters\n",
    "        grad_W = cost_wrt_W(cost_grad_deltas, x)\n",
    "        grad_b = cost_wrt_b(cost_grad_deltas)\n",
    "\n",
    "        #4. Update the model, we update with the mean gradient\n",
    "        # over the minibatch, rather than sum of particular gradients\n",
    "        # in a minibatch, to do so we scale the learning rate by batch_size\n",
    "        batch_size = x.shape[0]\n",
    "        effect_learn_rate = learning_rate / batch_size\n",
    "\n",
    "        W = W - effect_learn_rate * grad_W\n",
    "        b = b - effect_learn_rate * grad_b\n",
    "    \n",
    "    return W, b, numpy.mean(mse_stats)\n",
    "\n",
    "def sgd(data_provider, W, b, learning_rate=0.1, max_epochs=10):\n",
    "    \n",
    "    for epoch in xrange(0, max_epochs):\n",
    "        #reset the data provider\n",
    "        data_provider.reset()\n",
    "        \n",
    "        #train for one epoch\n",
    "        W, b, mean_cost = \\\n",
    "            sgd_epoch(data_provider, W, b, learning_rate)\n",
    "                \n",
    "        print \"MSE training cost after %d-th epoch is %f\" % (epoch + 1, mean_cost)\n",
    "    \n",
    "    return W, b\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE training cost after 1-th epoch is 0.306069\n",
      "MSE training cost after 2-th epoch is 0.255906\n",
      "MSE training cost after 3-th epoch is 0.244427\n",
      "MSE training cost after 4-th epoch is 0.241125\n",
      "MSE training cost after 5-th epoch is 0.240180\n",
      "MSE training cost after 6-th epoch is 0.239903\n",
      "MSE training cost after 7-th epoch is 0.239818\n",
      "MSE training cost after 8-th epoch is 0.239790\n",
      "MSE training cost after 9-th epoch is 0.239779\n",
      "MSE training cost after 10-th epoch is 0.239775\n",
      "MSE training cost after 11-th epoch is 0.239773\n",
      "MSE training cost after 12-th epoch is 0.239772\n",
      "MSE training cost after 13-th epoch is 0.239771\n",
      "MSE training cost after 14-th epoch is 0.239771\n",
      "MSE training cost after 15-th epoch is 0.239771\n",
      "MSE training cost after 16-th epoch is 0.239771\n",
      "MSE training cost after 17-th epoch is 0.239771\n",
      "MSE training cost after 18-th epoch is 0.239771\n",
      "MSE training cost after 19-th epoch is 0.239771\n",
      "MSE training cost after 20-th epoch is 0.239771\n",
      "MSE training cost after 21-th epoch is 0.239771\n",
      "MSE training cost after 22-th epoch is 0.239771\n",
      "MSE training cost after 23-th epoch is 0.239771\n",
      "MSE training cost after 24-th epoch is 0.239771\n",
      "MSE training cost after 25-th epoch is 0.239771\n",
      "MSE training cost after 26-th epoch is 0.239771\n",
      "MSE training cost after 27-th epoch is 0.239771\n",
      "MSE training cost after 28-th epoch is 0.239771\n",
      "MSE training cost after 29-th epoch is 0.239771\n",
      "MSE training cost after 30-th epoch is 0.239771\n",
      "MSE training cost after 31-th epoch is 0.239771\n",
      "MSE training cost after 32-th epoch is 0.239771\n",
      "MSE training cost after 33-th epoch is 0.239771\n",
      "MSE training cost after 34-th epoch is 0.239771\n",
      "MSE training cost after 35-th epoch is 0.239771\n",
      "MSE training cost after 36-th epoch is 0.239771\n",
      "MSE training cost after 37-th epoch is 0.239771\n",
      "MSE training cost after 38-th epoch is 0.239771"
     ]
    }
   ],
   "source": [
    "#some hyper-parameters\n",
    "window_size = 12\n",
    "irange = 0.5\n",
    "learning_rate = 0.01\n",
    "max_epochs=40\n",
    "\n",
    "# note, while developing you can set max_num_batches to some positive number to limit\n",
    "# the number of training data-points (you will get feedback faster)\n",
    "mdp = MetOfficeDataProvider(window_size, batch_size=10, max_num_batches=-100, randomize=False)\n",
    "\n",
    "#initialise the parameters\n",
    "W = random_generator.uniform(-irange, irange, (window_size, 1))\n",
    "b = random_generator.uniform(-irange, irange, (1, ))\n",
    "\n",
    "#train the model\n",
    "sgd(mdp, W, b, learning_rate=learning_rate, max_epochs=max_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 5\n",
    "\n",
    "Modify the above prediction (regression) problem so the model makes a binary classification whether the the weather is going to be one of those \\{rainy, not-rainy} (look at slide 12 of the 2nd lecture)\n",
    "\n",
    "Tip: You need to introduce the following changes:\n",
    "1. Modify `MetOfficeDataProvider` (for example, inherit from MetOfficeDataProvider to create a new class MetOfficeDataProviderBin) and modify `next()` function so it returns as `targets` either 0 (not-rainy - if the the amount of rain [before mean/variance normalisation] is equal to 0) or 1 (rainy -- otherwise).\n",
    "2. Modify the functions from previous exercise so the fprop implements `sigmoid` on top of affine transform.\n",
    "3. Modify cost function to binary cross-entropy\n",
    "4. Make sure you compute the gradients correctly (as you have changed both the output and the cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
