{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework #1\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This coursework is concerned with building multi-layer networks to address the MNIST digit classification problem. It builds on the previous labs, in particular [02_MNIST_SLN.ipynb](02_MNIST_SLN.ipynb) in which single layer networks were trained for MNIST digit classification.   The course will involve extending that code to use Sigmoid and Softmax layers, combining these into multi-layer networks, and carrying out a number of MNIST digit classification experiments, to investigate the effect of learning rate, the number of hidden units, and the number of hidden layers.\n",
    "\n",
    "The coursework is divided into 4 tasks:\n",
    "* **Task 1**:   *Implementing a sigmoid layer* - 15 marks.  \n",
    "This task involves extending the `Linear` class in file `mlp/layers.py` to `Sigmoid`, with code for forward prop, backprop computation of the gradient, and weight update.\n",
    "* **Task 2**:  *Implementing a softmax layer* - 15 marks.  \n",
    "This task involves extending the `Linear` class in file `mlp/layers.py` to `Softmax`, with code for forward prop, backprop computation of the gradient, and weight update.\n",
    "* **Task 3**:  *Constructing a multi-layer network* - 40 marks.  \n",
    "This task involves putting together a Sigmoid and a Softmax layer to create a multi-layer network, with one hidden layer (100 units) and one output layer, that is trained to classify MNIST digits.  This task will include reporting classification results, exploring the effect of learning rates, and plotting Hinton Diagrams for the hidden units and output units.\n",
    "* **Task 4**:  *Experiments with different architectures*  - 30 marks.  \n",
    "This task involves further MNIST classification experiments, primarily looking at the effect of using different numbers of hidden layers.\n",
    "The coursework will be marked out of 100, and will contribute 30% of the total mark in the MLP course.\n",
    "\n",
    "## Previous Tutorials\n",
    "\n",
    "Before starting this coursework make sure that you have completed the first three labs:\n",
    "\n",
    "* [00_Introduction.ipynb](00_Introduction.ipynb) - setting up your environment; *Solutions*: [00_Introduction_solution.ipynb](00_Introduction_solution.ipynb)\n",
    "* [01_Linear_Models.ipynb](01_Linear_Models.ipynb) - training single layer networks; *Solutions*: [01_Linear_Models_solution.ipynb](01_Linear_Models_solution.ipynb)\n",
    "* [02_MNIST_SLN.ipynb](02_MNIST_SLN.ipynb) - training a single layer network for MNIST digit classification\n",
    "\n",
    "To ensure that your virtual environment is correct, please see [this note](https://github.com/CSTR-Edinburgh/mlpractical/blob/master/kernel_issue_fix.md) on the GitHub.\n",
    "## Submission\n",
    "**Submission Deadline:  Thursday 29 October, 16:00** \n",
    "\n",
    "Submit the coursework as an ipython notebook file, using the `submit` command in the terminal on a DICE machine. If your file is `03_MLP_Coursework1.ipynb` then you would enter:\n",
    "\n",
    "`submit mlp 1 03_MLP_Coursework1.ipynb` \n",
    "\n",
    "where `mlp 1` indicates this is the first coursework of MLP.\n",
    "\n",
    "After submitting, you should receive an email of acknowledgment from the system confirming that your submission has been received successfully. Keep the email as evidence of your coursework submission.\n",
    "\n",
    "**Please make sure you submit a single `ipynb` file (and nothing else)!**\n",
    "\n",
    "**Submission Deadline:  Thursday 29 October, 16:00** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Please enter your exam number and the date in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MLP Coursework 1\n",
    "#Exam number: <ENTER EXAM NUMBER>\n",
    "#Date: <ENTER DATE>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the next code cell, which imports `numpy` and seeds the random number generator.  Please **do not** modify the random number generator seed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "#Seed a random number generator running the below cell, but do **not** modify the seed.\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "rng_state = rng.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Sigmoid Layer (15%)\n",
    "\n",
    "In this task you need to create a class `Sigmoid` which encapsulates a layer of sigmoid units.  You should do this by extending the `mlp.layers.Linear` class (in file `mlp/layers.py`), which implements a a layer of linear units (i.e. weighted sum plus bias).  The `Sigmoid` class extends this by applying the sigmoid transfer function to the weighted sum in the forward propagation, and applying the derivative of the sigmoid in the gradient descent back propagation and computing the gradients with respect to layer's parameters. Do **not** copy the implementation provided in `Linear` class but rather, **reuse** it through inheritance.\n",
    "\n",
    "When you have implemented `Sigmoid` (in the `mlp.layers` module), then please test it by running the below code cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.21941537867\n",
      "25.9255147706\n",
      "4.1105500626\n",
      "[ 0.067  0.728  0.999  0.512  0.159  0.584  0.238  0.932]\n",
      "[ -1.263e+00   1.037e+01   0.000e+00   1.249e-02   6.678e-03   1.191e+01\n",
      "   3.628e+00   1.268e+00]\n",
      "[ 1.406  0.078 -0.268  0.418  1.646  0.831]\n"
     ]
    }
   ],
   "source": [
    "from mlp.layers import Sigmoid\n",
    "\n",
    "a = numpy.asarray([-20.1, 52.4, 0, 0.05, 0.05, 49])\n",
    "b = numpy.asarray([-20.1, 52.4, 0, 0.05, 0.05, 49, 20, 20])\n",
    "\n",
    "rng.set_state(rng_state)\n",
    "sigm = Sigmoid(idim=a.shape[0], odim=b.shape[0], rng=rng) # idim = 6, odim = 8\n",
    "\n",
    "fp = sigm.fprop(a)\n",
    "deltas, ograds  = sigm.bprop(h=fp, igrads=b)\n",
    "\n",
    "print fp.sum()\n",
    "print deltas.sum()\n",
    "print ograds.sum()\n",
    "%precision 3\n",
    "print fp\n",
    "print deltas\n",
    "print ograds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "To include the `Sigmoid` code in the notebook please run the below code cell.  (The `%load` notebook command is used to load the source of the `Sigmoid` class from `mlp/layers.py`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load -s Sigmoid mlp/layers.py\n",
    "class Sigmoid(Linear):\n",
    "    def __init__(self,  idim, odim,\n",
    "                 rng=None,\n",
    "                 irange=0.1):\n",
    "\n",
    "        super(Sigmoid, self).__init__(idim, odim, rng, irange)\n",
    "    \n",
    "    def fprop(self, inputs):\n",
    "        #get the linear activations\n",
    "        a = super(Sigmoid, self).fprop(inputs) # Inheritate from Linear class\n",
    "        #stabilise the exp() computation in case some values in\n",
    "        #'a' get very negative. We limit both tails, however only\n",
    "        #negative values may lead to numerical issues -- exp(-a)\n",
    "        #clip() function does the following operation faster:\n",
    "        # a[a < -30.] = -30,\n",
    "        # a[a > 30.] = 30.\n",
    "        numpy.clip(a, -30.0, 30.0, out=a)\n",
    "        h = 1.0/(1 + numpy.exp(-a))\n",
    "        return h\n",
    "    \n",
    "    def bprop(self, h, igrads):\n",
    "        dsigm = h * (1.0 - h)\n",
    "        deltas = igrads * dsigm\n",
    "        ___, ograds = super(Sigmoid, self).bprop(h=None, igrads=deltas)\n",
    "        return deltas, ograds\n",
    "\n",
    "    def cost_bprop(self, h, igrads, cost):\n",
    "        if cost is None or cost.get_name() == 'bce':\n",
    "            return super(Sigmoid, self).bprop(h=h, igrads=igrads)\n",
    "        else:\n",
    "            raise NotImplementedError('Sigmoid.bprop_cost method not implemented '\n",
    "                                      'for the %s cost' % cost.get_name())\n",
    "\n",
    "    def get_name(self):\n",
    "        return 'sigmoid'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Softmax (15%)\n",
    "\n",
    "In this task you need to create a class `Softmax` which encapsulates a layer of softmax units.  As in the previous task, you should do this by extending the `mlp.layers.Linear` class (in file `mlp/layers.py`).\n",
    "\n",
    "When you have implemented `Softmax` (in the `mlp.layers` module), then please test it by running the below code cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "-1.11022302463e-16\n",
      "0.0744177068753\n",
      "[  4.571e-05   1.697e-03   9.877e-01   6.631e-04   1.194e-04   8.880e-04\n",
      "   1.977e-04   8.671e-03]\n",
      "[  4.571e-05   1.697e-03   9.877e-01   6.631e-04   1.194e-04   8.880e-04\n",
      "   1.977e-04  -9.913e-01]\n",
      "[-0.089  0.03   0.079  0.011  0.017  0.027]\n"
     ]
    }
   ],
   "source": [
    "from mlp.layers import Softmax\n",
    "\n",
    "a = numpy.asarray([-20.1, 52.4, 0, 0.05, 0.05, 49])\n",
    "b = numpy.asarray([0, 0, 0, 0, 0, 0, 0, 1])\n",
    "\n",
    "rng.set_state(rng_state)\n",
    "softmax = Softmax(idim=a.shape[0], odim=b.shape[0], rng=rng)\n",
    "\n",
    "fp = softmax.fprop(a)\n",
    "deltas, ograds = softmax.bprop_cost(h=None, igrads=fp-b, cost=None)\n",
    "\n",
    "print fp.sum()\n",
    "print deltas.sum()\n",
    "print ograds.sum()\n",
    "%precision 3\n",
    "print fp\n",
    "print deltas\n",
    "print ograds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "To include the `Softmax` code in the notebook please run the below code cell.  (The notebook `%load` command is used to load the source of the `Softmax` class from `mlp/layers.py`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load -s Softmax mlp/layers.py\n",
    "class Softmax(Linear):\n",
    "\n",
    "    def __init__(self,idim, odim,\n",
    "                 rng=None,\n",
    "                 irange=0.1):\n",
    "\n",
    "        super(Softmax, self).__init__(idim,\n",
    "                                      odim,\n",
    "                                      rng=rng,\n",
    "                                      irange=irange)\n",
    "    \n",
    "    def fprop(self, inputs):\n",
    "\n",
    "        # compute the linear outputs\n",
    "        a = super(Softmax, self).fprop(inputs)\n",
    "        # apply numerical stabilisation by subtracting max \n",
    "        # from each row (not required for the coursework)\n",
    "        # then compute exponent\n",
    "        assert a.ndim in [1, 2], (\n",
    "            \"Expected the linear activation in Softmax layer to be either \"\n",
    "            \"vector or matrix, got %ith dimensional tensor\" % a.ndim\n",
    "        )\n",
    "        axis = a.ndim - 1\n",
    "        exp_a = numpy.exp(a - numpy.max(a, axis=axis, keepdims=True))\n",
    "        # finally, normalise by the sum within each example\n",
    "        y = exp_a/numpy.sum(exp_a, axis=axis, keepdims=True)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def bprop(self, h, igrads):\n",
    "        raise NotImplementedError('Softmax.bprop not implemented for hidden layer.')\n",
    "\n",
    "    def bprop_cost(self, h, igrads, cost):\n",
    "\n",
    "        if cost is None or cost.get_name() == 'ce':\n",
    "            return super(Softmax, self).bprop(h=h, igrads=igrads)\n",
    "        else:\n",
    "            raise NotImplementedError('Softmax.bprop_cost method not implemented '\n",
    "                                      'for %s cost' % cost.get_name())\n",
    "\n",
    "    def get_name(self):\n",
    "        return 'softmax'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Multi-layer network for MNIST classification (40%)\n",
    "\n",
    "**(a)** (20%)  Building on the single layer linear network for MNIST classification used in lab [02_MNIST_SLN.ipynb](02_MNIST_SLN.ipynb), and using the `Sigmoid` and `Softmax` classes that you implemented in tasks 1 and 2, construct and learn a model that classifies MNIST images and:\n",
    "   * Has one hidden layer with a sigmoid transfer function and 100 units\n",
    "   * Uses a softmax output layer to discriminate between the 10 digit classes (use the `mlp.costs.CECost()` cost)\n",
    "\n",
    "Your code should print the final values of the error function and the classification accuracy for train, validation, and test sets (please keep also the log information printed by default by the optimiser). Limit the number of training epochs to 30. You can, of course, split your code across as many cells as you think is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import logging\n",
    "\n",
    "from mlp.layers import MLP, Linear, Sigmoid, Softmax #import required layer types\n",
    "from mlp.optimisers import SGDOptimiser #import the optimiser\n",
    "from mlp.dataset import MNISTDataProvider #import data provider\n",
    "from mlp.costs import MSECost, CECost #import the cost we want to use for optimisation\n",
    "from mlp.schedulers import LearningRateFixed\n",
    "\n",
    "#Seed a random number generator running the below cell, but do **not** modify the seed.\n",
    "rng = numpy.random.RandomState([2015,10,10])\n",
    "rng_state = rng.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialising data providers...\n",
      "INFO:root:Training started...\n",
      "INFO:mlp.optimisers:Epoch 0: Training cost (ce) for initial model is 2.348. Accuracy is 7.56%\n",
      "INFO:mlp.optimisers:Epoch 0: Validation cost (ce) for initial model is 2.350. Accuracy is 7.23%\n",
      "INFO:mlp.optimisers:Epoch 1: Training cost (ce) is 1.957. Accuracy is 53.41%\n",
      "INFO:mlp.optimisers:Epoch 1: Validation cost (ce) is 1.560. Accuracy is 73.99%\n",
      "INFO:mlp.optimisers:Epoch 1: Took 27 seconds. Training speed 1937 pps. Validation speed 6526 pps.\n",
      "INFO:mlp.optimisers:Epoch 2: Training cost (ce) is 1.262. Accuracy is 76.52%\n",
      "INFO:mlp.optimisers:Epoch 2: Validation cost (ce) is 0.982. Accuracy is 82.91%\n",
      "INFO:mlp.optimisers:Epoch 2: Took 30 seconds. Training speed 1767 pps. Validation speed 5981 pps.\n",
      "INFO:mlp.optimisers:Epoch 3: Training cost (ce) is 0.866. Accuracy is 82.55%\n",
      "INFO:mlp.optimisers:Epoch 3: Validation cost (ce) is 0.712. Accuracy is 85.85%\n",
      "INFO:mlp.optimisers:Epoch 3: Took 32 seconds. Training speed 1661 pps. Validation speed 6104 pps.\n",
      "INFO:mlp.optimisers:Epoch 4: Training cost (ce) is 0.676. Accuracy is 85.27%\n",
      "INFO:mlp.optimisers:Epoch 4: Validation cost (ce) is 0.576. Accuracy is 87.37%\n",
      "INFO:mlp.optimisers:Epoch 4: Took 31 seconds. Training speed 1717 pps. Validation speed 5995 pps.\n",
      "INFO:mlp.optimisers:Epoch 5: Training cost (ce) is 0.570. Accuracy is 86.72%\n",
      "INFO:mlp.optimisers:Epoch 5: Validation cost (ce) is 0.495. Accuracy is 88.76%\n",
      "INFO:mlp.optimisers:Epoch 5: Took 31 seconds. Training speed 1693 pps. Validation speed 6106 pps.\n",
      "INFO:mlp.optimisers:Epoch 6: Training cost (ce) is 0.504. Accuracy is 87.89%\n",
      "INFO:mlp.optimisers:Epoch 6: Validation cost (ce) is 0.443. Accuracy is 89.25%\n",
      "INFO:mlp.optimisers:Epoch 6: Took 31 seconds. Training speed 1714 pps. Validation speed 5996 pps.\n",
      "INFO:mlp.optimisers:Epoch 7: Training cost (ce) is 0.459. Accuracy is 88.54%\n",
      "INFO:mlp.optimisers:Epoch 7: Validation cost (ce) is 0.407. Accuracy is 89.92%\n",
      "INFO:mlp.optimisers:Epoch 7: Took 31 seconds. Training speed 1713 pps. Validation speed 5984 pps.\n",
      "INFO:mlp.optimisers:Epoch 8: Training cost (ce) is 0.427. Accuracy is 89.07%\n",
      "INFO:mlp.optimisers:Epoch 8: Validation cost (ce) is 0.380. Accuracy is 90.34%\n",
      "INFO:mlp.optimisers:Epoch 8: Took 31 seconds. Training speed 1695 pps. Validation speed 5987 pps.\n",
      "INFO:mlp.optimisers:Epoch 9: Training cost (ce) is 0.403. Accuracy is 89.46%\n",
      "INFO:mlp.optimisers:Epoch 9: Validation cost (ce) is 0.360. Accuracy is 90.64%\n",
      "INFO:mlp.optimisers:Epoch 9: Took 31 seconds. Training speed 1720 pps. Validation speed 5922 pps.\n",
      "INFO:mlp.optimisers:Epoch 10: Training cost (ce) is 0.383. Accuracy is 89.83%\n",
      "INFO:mlp.optimisers:Epoch 10: Validation cost (ce) is 0.345. Accuracy is 90.78%\n",
      "INFO:mlp.optimisers:Epoch 10: Took 31 seconds. Training speed 1699 pps. Validation speed 5914 pps.\n",
      "INFO:mlp.optimisers:Epoch 11: Training cost (ce) is 0.368. Accuracy is 90.06%\n",
      "INFO:mlp.optimisers:Epoch 11: Validation cost (ce) is 0.331. Accuracy is 91.03%\n",
      "INFO:mlp.optimisers:Epoch 11: Took 32 seconds. Training speed 1651 pps. Validation speed 5890 pps.\n",
      "INFO:mlp.optimisers:Epoch 12: Training cost (ce) is 0.355. Accuracy is 90.31%\n",
      "INFO:mlp.optimisers:Epoch 12: Validation cost (ce) is 0.320. Accuracy is 91.37%\n",
      "INFO:mlp.optimisers:Epoch 12: Took 32 seconds. Training speed 1675 pps. Validation speed 5816 pps.\n",
      "INFO:mlp.optimisers:Epoch 13: Training cost (ce) is 0.344. Accuracy is 90.57%\n",
      "INFO:mlp.optimisers:Epoch 13: Validation cost (ce) is 0.311. Accuracy is 91.56%\n",
      "INFO:mlp.optimisers:Epoch 13: Took 32 seconds. Training speed 1640 pps. Validation speed 5775 pps.\n",
      "INFO:mlp.optimisers:Epoch 14: Training cost (ce) is 0.334. Accuracy is 90.78%\n",
      "INFO:mlp.optimisers:Epoch 14: Validation cost (ce) is 0.302. Accuracy is 91.81%\n",
      "INFO:mlp.optimisers:Epoch 14: Took 32 seconds. Training speed 1668 pps. Validation speed 5478 pps.\n",
      "INFO:mlp.optimisers:Epoch 15: Training cost (ce) is 0.325. Accuracy is 90.94%\n",
      "INFO:mlp.optimisers:Epoch 15: Validation cost (ce) is 0.295. Accuracy is 91.96%\n",
      "INFO:mlp.optimisers:Epoch 15: Took 32 seconds. Training speed 1666 pps. Validation speed 5225 pps.\n",
      "INFO:mlp.optimisers:Epoch 16: Training cost (ce) is 0.318. Accuracy is 91.13%\n",
      "INFO:mlp.optimisers:Epoch 16: Validation cost (ce) is 0.289. Accuracy is 92.15%\n",
      "INFO:mlp.optimisers:Epoch 16: Took 32 seconds. Training speed 1645 pps. Validation speed 5742 pps.\n",
      "INFO:mlp.optimisers:Epoch 17: Training cost (ce) is 0.311. Accuracy is 91.32%\n",
      "INFO:mlp.optimisers:Epoch 17: Validation cost (ce) is 0.283. Accuracy is 92.22%\n",
      "INFO:mlp.optimisers:Epoch 17: Took 32 seconds. Training speed 1660 pps. Validation speed 5719 pps.\n",
      "INFO:mlp.optimisers:Epoch 18: Training cost (ce) is 0.304. Accuracy is 91.42%\n",
      "INFO:mlp.optimisers:Epoch 18: Validation cost (ce) is 0.277. Accuracy is 92.42%\n",
      "INFO:mlp.optimisers:Epoch 18: Took 32 seconds. Training speed 1665 pps. Validation speed 5527 pps.\n",
      "INFO:mlp.optimisers:Epoch 19: Training cost (ce) is 0.298. Accuracy is 91.64%\n",
      "INFO:mlp.optimisers:Epoch 19: Validation cost (ce) is 0.272. Accuracy is 92.43%\n",
      "INFO:mlp.optimisers:Epoch 19: Took 32 seconds. Training speed 1662 pps. Validation speed 5568 pps.\n",
      "INFO:mlp.optimisers:Epoch 20: Training cost (ce) is 0.293. Accuracy is 91.75%\n",
      "INFO:mlp.optimisers:Epoch 20: Validation cost (ce) is 0.268. Accuracy is 92.41%\n",
      "INFO:mlp.optimisers:Epoch 20: Took 32 seconds. Training speed 1646 pps. Validation speed 5972 pps.\n",
      "INFO:mlp.optimisers:Epoch 21: Training cost (ce) is 0.288. Accuracy is 91.87%\n",
      "INFO:mlp.optimisers:Epoch 21: Validation cost (ce) is 0.263. Accuracy is 92.56%\n",
      "INFO:mlp.optimisers:Epoch 21: Took 32 seconds. Training speed 1650 pps. Validation speed 5605 pps.\n",
      "INFO:mlp.optimisers:Epoch 22: Training cost (ce) is 0.283. Accuracy is 91.97%\n",
      "INFO:mlp.optimisers:Epoch 22: Validation cost (ce) is 0.259. Accuracy is 92.70%\n",
      "INFO:mlp.optimisers:Epoch 22: Took 33 seconds. Training speed 1627 pps. Validation speed 5574 pps.\n",
      "INFO:mlp.optimisers:Epoch 23: Training cost (ce) is 0.279. Accuracy is 92.08%\n",
      "INFO:mlp.optimisers:Epoch 23: Validation cost (ce) is 0.256. Accuracy is 92.82%\n",
      "INFO:mlp.optimisers:Epoch 23: Took 32 seconds. Training speed 1634 pps. Validation speed 5601 pps.\n",
      "INFO:mlp.optimisers:Epoch 24: Training cost (ce) is 0.274. Accuracy is 92.23%\n",
      "INFO:mlp.optimisers:Epoch 24: Validation cost (ce) is 0.252. Accuracy is 92.88%\n",
      "INFO:mlp.optimisers:Epoch 24: Took 32 seconds. Training speed 1652 pps. Validation speed 5628 pps.\n",
      "INFO:mlp.optimisers:Epoch 25: Training cost (ce) is 0.270. Accuracy is 92.31%\n",
      "INFO:mlp.optimisers:Epoch 25: Validation cost (ce) is 0.249. Accuracy is 92.96%\n",
      "INFO:mlp.optimisers:Epoch 25: Took 32 seconds. Training speed 1638 pps. Validation speed 5552 pps.\n",
      "INFO:mlp.optimisers:Epoch 26: Training cost (ce) is 0.266. Accuracy is 92.45%\n",
      "INFO:mlp.optimisers:Epoch 26: Validation cost (ce) is 0.246. Accuracy is 93.07%\n",
      "INFO:mlp.optimisers:Epoch 26: Took 32 seconds. Training speed 1653 pps. Validation speed 5564 pps.\n",
      "INFO:mlp.optimisers:Epoch 27: Training cost (ce) is 0.263. Accuracy is 92.53%\n",
      "INFO:mlp.optimisers:Epoch 27: Validation cost (ce) is 0.242. Accuracy is 93.14%\n",
      "INFO:mlp.optimisers:Epoch 27: Took 32 seconds. Training speed 1650 pps. Validation speed 5656 pps.\n",
      "INFO:mlp.optimisers:Epoch 28: Training cost (ce) is 0.259. Accuracy is 92.59%\n",
      "INFO:mlp.optimisers:Epoch 28: Validation cost (ce) is 0.239. Accuracy is 93.20%\n",
      "INFO:mlp.optimisers:Epoch 28: Took 32 seconds. Training speed 1643 pps. Validation speed 5685 pps.\n",
      "INFO:mlp.optimisers:Epoch 29: Training cost (ce) is 0.256. Accuracy is 92.68%\n",
      "INFO:mlp.optimisers:Epoch 29: Validation cost (ce) is 0.237. Accuracy is 93.34%\n",
      "INFO:mlp.optimisers:Epoch 29: Took 32 seconds. Training speed 1650 pps. Validation speed 5614 pps.\n",
      "INFO:mlp.optimisers:Epoch 30: Training cost (ce) is 0.252. Accuracy is 92.76%\n",
      "INFO:mlp.optimisers:Epoch 30: Validation cost (ce) is 0.234. Accuracy is 93.30%\n",
      "INFO:mlp.optimisers:Epoch 30: Took 32 seconds. Training speed 1634 pps. Validation speed 5539 pps.\n",
      "INFO:root:Testing the model on test set:\n",
      "INFO:root:MNIST test set accuracy is 93.24 % (cost is 0.243)\n"
     ]
    }
   ],
   "source": [
    "# include here the complete code that constructs the model, performs training,\n",
    "# and prints the error and accuracy for train/valid/test\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "lr_scheduler = LearningRateFixed(learning_rate=0.01, max_epochs=30)\n",
    "optimiser = SGDOptimiser(lr_scheduler=lr_scheduler)\n",
    "\n",
    "model = MLP(cost=CECost())\n",
    "model.set_layers([Linear(idim=784, odim=784, rng=rng), Sigmoid(idim = 784, odim = 100, rng = rng), \\\n",
    "                  Softmax(idim = 100, odim = 10, rng = rng)])\n",
    "logger.info('Initialising data providers...')\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=-10, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=100, max_num_batches=-10, randomize=False)\n",
    "\n",
    "logger.info('Training started...')\n",
    "optimiser.train(model, train_dp, valid_dp)\n",
    "\n",
    "logger.info('Testing the model on test set:')\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=100, max_num_batches=-10, randomize=False)\n",
    "cost, accuracy = optimiser.validate(model, test_dp)\n",
    "logger.info('MNIST test set accuracy is %.2f %% (cost is %.3f)'%(accuracy*100., cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)** (10%) Investigate the impact of different learning rates $\\eta \\in \\{0.5, 0.2, 0.1, 0.05, 0.01, 0.005\\}$ on the convergence of the network training as well as the final accuracy:\n",
    "   * Plot (on a single graph) the error rate curves for each learning rate as a function of training epochs for training set\n",
    "   * Plot (on another single graph) the error rate curves as a function of training epochs for validation set\n",
    "   * Include a table of the corresponding error rates for test set\n",
    "\n",
    "The notebook command `%matplotlib inline` ensures that your graphs will be added to the notebook, rather than opened as additional windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eta = numpy.array([0.005, 0.01, 0.05, 0.1, 0.2, 0.5])\n",
    "max_number_ephocs = 30\n",
    "\n",
    "train_errors = numpy.zeros((max_number_ephocs, numpy.shape(eta)[0]), dtype = float)\n",
    "valid_errors = numpy.zeros_like(train_errors)\n",
    "test_errors = numpy.zeros(numpy.shape(eta)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_scheduler = LearningRateFixed(learning_rate=0.01, max_epochs=30)\n",
    "train_dp = MNISTDataProvider(dset='train', batch_size=100, max_num_batches=-10, randomize=True)\n",
    "valid_dp = MNISTDataProvider(dset='valid', batch_size=100, max_num_batches=-10, randomize=False)\n",
    "test_dp = MNISTDataProvider(dset='eval', batch_size=100, max_num_batches=-10, randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for jj in eta:\n",
    "    model = MLP(cost=CECost())\n",
    "    model.set_layers([Linear(idim=784, odim=784, rng=rng), Sigmoid(idim = 784, odim = 100, rng = rng), \\\n",
    "                  Softmax(idim = 100, odim = 10, rng = rng)])\n",
    "    optimiser.train(model, train_dp, valid_dp)\n",
    "    cost, accuracy = optimiser.validate(model, test_dp)\n",
    "    numpy.append(test_errors) = cost\n",
    "    # Need to save training and validation error in each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** (10%) Plot the following graphs:\n",
    "  * Display the 784-element weight vector of each of the 100 hidden units as 10x10 grid plot of 28x28 images, in order to visualise what features of the input they are encoding.  To do this, take the weight vector of each hidden unit, reshape to 28x28, and plot using the `imshow` function).\n",
    "  * Plot a Hinton Diagram of the output layer weight matrix for digits 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 4 - Experiments with 1-5 hidden layers (30%)\n",
    "\n",
    "In this task use the learning rate which resulted in the best accuracy in your experiments in Task 3 (b).  Perform the following experiments:\n",
    "\n",
    "  * Train a similar model to Task 3, with one hidden layer, but with 800 hidden units. \n",
    "  * Train 4 additional models with 2, 3, 4 and 5 hidden layers.  Set the number of hidden units for each model, such that all the models have similar number of trainable weights ($\\pm$2%).   For simplicity, for a given model, keep the number of units in each hidden layer the same.\n",
    "  * Plot value of the error function for training and validation sets as a function of training epochs for each model\n",
    "  * Plot the test set classification accuracy as a function of the number of hidden layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is the end of coursework 1.\n",
    "\n",
    "Please remember to save your notebook, and submit your notebook following the instructions at the top.  Please make sure that you have executed all the code cells when you submit the notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
